# GRPO 奖励计算说明 (Reward Calculation) - V2

在本项目中，我们使用混合奖励机制，结合了 **结果奖励 (Outcome Reward)** 和 **格式激励 (Format Incentive)**，以解决模型训练初期的冷启动问题。

## 1. 核心逻辑

总奖励 ($R$) 计算公式如下：
$$ R = R_{\text{correct}} + R_{\text{format}} $$

### 1.1 结果奖励 ($R_{\text{correct}}$)
*   **判定标准**: 从回复中提取 `####` 后的数值，并与标准答案进行严格比对。
*   **分值**: 正确得 **1.0**，错误得 **0.0**。

### 1.2 格式激励 ($R_{\text{format}}$) - **新引入**
为了防止模型在初期因为算不对且不会写格式而拿不到任何奖励（导致 `pg_loss` 为 0），我们引入了微小的格式奖励：
*   **判定标准 A**: 回复中包含 `####` 分隔符。
*   **分值**: **+0.05**
*   **判定标准 B**: `####` 后面紧跟着数字（即初步符合答案格式）。
*   **分值**: **+0.1** (替代标准 A)

**设计理由**: 引导模型先学会“说话的套路”（输出 `#### <数字>`），只要它学会了套路，哪怕算错了也能拿到 0.1 分。相对于拿 0 分的其他回复，这 0.1 分就是正优势，能驱动模型快速向正确格式收敛。

---

### 1.3 官方逻辑优化 (Official Alignment) - **新优化**
为了进一步对齐最佳实践，我们引入了官方 `verl` 实现中的两项关键优化：
*   **末尾截断 (Context Clipping)**: 匹配范围仅限于回复的 **最后 300 个字符**。这可以防止模型在推理中间过程误用格式，强制其在结尾给出结论。
*   **后验优先 (Last Match)**: 如果模型在回复中多次出现了 `####`，我们仅以 **最后一个** 为准。这允许模型在 Chain-of-Thought 中进行自我纠错（Self-Correction）。

---

## 2. 效果对比

| 场景 | 旧奖励 (V1) | 优化奖励 (V3) | 驱动效果 |
| :--- | :--- | :--- | :--- |
| 完全乱写 | 0.0 | 0.0 | 无 |
| 写了 `####` 但没写数字 | 0.0 | 0.05 | 鼓励使用分隔符 |
| 格式正确但算错 | 0.0 | 0.1 | 维持格式，鼓励生成数字 |
| 格式正确且算对 | 1.0 | 1.1 | 最终目标 |
| 中间写错结尾写对 | 0.0 | 1.1 | 鼓励自我纠错 |

## 3. 为什么 V1 会失败？
在 A100 训练中，如果 Base 模型能力较弱，采样 64 个回复可能全是 0 分。在 GRPO 中，如果 $G$ 个样本全是 0 分，那么它们的 Advantage 全是 0，模型参数将得不到任何有效更新，从而陷入死循环或输出废话。V2 的 0.1 分破除了这种“全 0 僵局”。